{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始构建网络：\n",
    "\n",
    "1.构建输入时间块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlockTime(torch.nn.Module):  #继承Module类的输入块类\n",
    "    def __init__(self,time_channels,time_size,spatial_channels,spatial_size,pool_param = (1,2),dropout = 0.5):\n",
    "        #这里写自己的魔法方法，这里的self指代的都是InputBlockTime这个类\n",
    "        super(InputBlockTime, self).__init__() #超类：调用父类的初始化方法\n",
    "        #输入16*1*64*256\n",
    "        \n",
    "        self.block  =torch.nn.Sequential( \n",
    "            #这是时间过滤器  \n",
    "            #输入16*1*64*256\n",
    "            torch.nn.Conv2d(in_channels=1,out_channels = time_channels,\n",
    "                            kernel_size=time_size,padding = (0,time_size[1]//2),bias = False),\n",
    "            #输出16*25*64*256,\n",
    "            #计算公式为 H = [(H-2*P+K_P)/S] + 1\n",
    "            \n",
    "            #这是空间过滤器\n",
    "            #输入16*25*64*256 \n",
    "            torch.nn.Conv2d(in_channels=time_channels,groups = time_channels, #这里的groups分组卷积25\n",
    "                            out_channels=spatial_channels,kernel_size = spatial_size),\n",
    "            #输出16*50*1*256\n",
    "            #批量归一化，防止数据过大\n",
    "            torch.nn.BatchNorm2d(spatial_channels),\n",
    "            #卷积中应该是不带激活函数的\n",
    "            torch.nn.ELU(), #激活函数\n",
    "            #最大池化\n",
    "            torch.nn.MaxPool2d(kernel_size=pool_param,stride= pool_param),\n",
    "            #输出 16*50*1*128\n",
    "            #计算公式为：H = (H-K_H)/S +1\n",
    "\n",
    "            #退却\n",
    "            torch.nn.Dropout(dropout) \n",
    "            #用来防止过拟合，把一部分值置为很小接近于0\n",
    "        )\n",
    "        #这里创建自己的成员block\n",
    "    def forward(self,x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是分组卷积的图示，参数量减少为1/G\n",
    "![](2023-02-19-17-22-27.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建特征提取块：\n",
    "\n",
    "这里我非常困惑的点是，为什么不能像之前那个块一样，直接用torch.Sequetial来构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBlock(torch.nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,conv_param = (1,15),pool_param = (1,2),dropout = 0.5,padding = (-1,-1,-1,-1)):\n",
    "        super(FeatureBlock, self).__init__() #是因为torch.nn.Module本来就没有传入参数\n",
    "        \n",
    "        if(padding[0]==-1): #这一步我并不知道是什么意义，改写了padding元组，\n",
    "            padding = (conv_param[1]//2,conv_param[1]//2,0,0)\n",
    "            #应该是单边的添零\n",
    "\n",
    "        #这一步是四周填0,分别是左右上下\n",
    "        self.padding = torch.nn.ZeroPad2d(padding = padding)\n",
    "        self.conv = torch.nn.Conv2d(in_channels = in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    kernel_size=conv_param,\n",
    "                                    bias = False)\n",
    "        #这一步不是很理解为什么num_features填这个值\n",
    "        self.norm = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        #这个池化，使得步长和卷积核相同是，输出尺寸为H/S\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size = pool_param,\n",
    "                                        stride = pool_param)\n",
    "        self.drop_out = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #其实也就是，填充->卷积->批量归一化->激活->池化 \n",
    "        #和前面那个模块感觉不出来区别\n",
    "        x = self.padding(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool(x)\n",
    "        return self.drop_out(x)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建深度卷积网络：\n",
    "\n",
    "这里我很困惑的点是，为什么time_channels,time_size,spatial_channels,spatial_size,feature_pool_size,feature_channels_list这样子取值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet(torch.nn.Module):\n",
    "    def __init__(self,sample,class_num,time_channels,time_size,\n",
    "                spatial_channels,spatial_size,\n",
    "                feature_pool_size,feature_channels_list,\n",
    "                dropout):\n",
    "        super(DeepConvNet, self).__init__()\n",
    "        #首先设置一个输入模块，用原来的输入模块，并传参\n",
    "        self.input_block = InputBlockTime(time_channels=time_channels,time_size=time_size,\n",
    "                                        spatial_channels=spatial_channels,spatial_size=spatial_size,\n",
    "                                        pool_param=feature_pool_size,\n",
    "                                        dropout = dropout)\n",
    "        self.feature_block_list = torch.nn.Sequential()\n",
    "        #以下通道赋值，是用来衔接输入模块，和特征提取模块\n",
    "        pre_channels = spatial_channels\n",
    "        for channel in feature_channels_list:\n",
    "            self.feature_block_list.add_module(\n",
    "                f\"feature {channel}\",  #第一个参数是名字，这里叫feature100\n",
    "                FeatureBlock(in_channels=pre_channels,out_channels=channel,\n",
    "                            pool_param=feature_pool_size,dropout=dropout)            \n",
    "            )\n",
    "            #这个是用来衔接两个特征提取模块\n",
    "            pre_channels = channel\n",
    "\n",
    "        #造一个数据来得到全连接层前数据的shape\n",
    "        #不用手动计算了的技巧\n",
    "        tmp_data = torch.Tensor(np.ones((1,1,64,sample),dtype=float))\n",
    "        tmp_data = self.input_block(tmp_data)\n",
    "        tmp_data = self.feature_block_list(tmp_data)\n",
    "        #这里是把tmp_data压扁，重新视为size(0) * 自适应长度\n",
    "        tmp_data = tmp_data.view(tmp_data.size(0),-1)\n",
    "        self.classifer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(tmp_data.shape[1],class_num)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out2 = self.input_block(x) #输入模块\n",
    "        x = self.feature_block_list(out2) #特征模块\n",
    "        x = x.view(x.size(0),-1) #压缩成条\n",
    "        x = self.classifer(x) #全连接层分类模块\n",
    "        return torch.nn.functional.softmax(x,dim =1) #利用softmax,在1维归一化\n",
    "        # return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lhjNet= DeepConvNet(\n",
    "        sample=256, class_num=2,\n",
    "        time_channels=25, time_size=(1, 9),\n",
    "        spatial_channels=50, spatial_size=(64, 1),\n",
    "        feature_pool_size=(1, 3), feature_channels_list=[100, 200], dropout=0.5)\n",
    "input =  np.ones((1,1,64,256)) \n",
    "input = torch.from_numpy(input).float()\n",
    "output = lhjNet(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-02-19-17-53-13.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是拆解运算 \n",
    "\n",
    "用来观察数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((1,1,64,256)) \n",
    "a = torch.from_numpy(a).float()\n",
    "#类的实例化，自动调用__init__方法\n",
    "class DeepConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepConvNet, self).__init__()\n",
    "        self.input_block = InputBlockTime(time_channels=25,\n",
    "                                            time_size = (1,9),\n",
    "                                            spatial_channels=50,\n",
    "                                            spatial_size=(64,1),\n",
    "                                            pool_param=(1,2),\n",
    "                                            dropout=0.5)\n",
    "        self.feature_block_list= torch.nn.Sequential()\n",
    "        pre_channels = 50\n",
    "        feature_channels_list = [100,200]\n",
    "        feature_pool_size = (1,3)\n",
    "        dropout = 0.5\n",
    "\n",
    "        for channel in feature_channels_list:\n",
    "            self.feature_block_list.add_module(\n",
    "                f\"feature {channel}\",  #第一个参数是名字，这里叫feature100\n",
    "                FeatureBlock(in_channels=pre_channels,out_channels=channel,\n",
    "                            pool_param=feature_pool_size,dropout=dropout)            \n",
    "            )\n",
    "            #这个是用来衔接两个特征提取模块\n",
    "            pre_channels = channel\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.input_block(x)\n",
    "        out = self.feature_block_list(out)\n",
    "        return out\n",
    "\n",
    "net = DeepConvNet()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b = net(a) #自动调用farward方法\n",
    "b = b.view(b.size(0),-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44ecba1be7491a5bc153ef91d6a574d9526dcd4a0fdd78aab2e1c84520780719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
